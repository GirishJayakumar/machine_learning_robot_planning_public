[environment]
agent_names = ['agent1']
renderer = my_renderer
steps_per_action = 1

[my_renderer]
type = Envmatplotlib
xaxis_range = [-15, 15]
yaxis_range = [-15, 15]
figure_size = [9, 9]
figure_dpi = 81
auto_range = False

[agent1]
type = abstract_robot
data_type = numpy
start_state = [-12.0, 8.0, 0.0, 0.0, 0.0]
dynamics = my_abstract_dynamics1
observer = my_abstract_observer1
cost_evaluator = my_abstract_cost_evaluator1
steps_per_action = 1
abstract_action_horizon = 5

[my_abstract_dynamics1]
type = abstract_dynamics
delta_t = 0.1
simulated_robot = my_simulated_robot1

[my_abstract_observer1]
type = abstract_full_state_observer

[my_abstract_cost_evaluator1]
type = abstract_cost
goal_checker = my_ultimate_goal_checker1
dense = False
non_achievable_cost = 500
achievable_cost = -500
ultimate_goal_cost = -5000

[my_ultimate_goal_checker1]
type = position_goal_checker
goal_state = [10.0, 5.0, 0.0, 0.0, 0.0]
goal_radius = 1
goal_color = r

[my_simulated_robot1]
type = simulated_robot
data_type = numpy
start_state = [0.0, 0.0, 0.0, 0.0, 0.0]
cost_evaluator = my_cost_evaluator1
dynamics = my_dynamics1
steps_per_action = 1
controller = my_controller1

[my_controller1]
type = MPPI
dynamics = my_dynamics1
cost_evaluator = my_cost_evaluator1
control_dim = 2
inverse_temperature = 1
control_horizon = 10
stochastic_trajectories_sampler = my_stochastic_trajectories_sampler1
warm_start_itr = 20

[my_stochastic_trajectories_sampler1]
# type = MPPI_parallel_stochastic_trajectories_sampler_multiprocessing
type = MPPI_stochastic_trajectories_sampler_slow_loop
number_of_trajectories = 100
uncontrolled_trajectories_portion = 0.0
noise_sampler = my_noise_sampler1

[my_noise_sampler1]
type = gaussian_noise_sampler
mean = [0, 0]
covariance = [[10, 0], [0, 10]]

[my_dynamics1]
type = bicycle_dynamics
mass = 1.0
cog_pos = 0.5
car_length = 1
delta_t = 0.1
state_bounds = [15, 15, 1.5, 3.14, 3]

[my_cost_evaluator1]
type = quadratic_cost
collision_cost = 1000
goal_cost = -5000
Q = [[1.0, 0.0, 0.0, 0.0, 0.0],[0.0, 1.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0, 0.0]]
R = [[1.0, 0.0],[0.0, 1.0]]
goal_checker = my_goal_checker1
collision_checker = my_collision_checker1

[my_goal_checker1]
# type = state_space_goal_checker
type = position_goal_checker
goal_state = [0.0, 0.0, 0.0, 0.0, 0.0]
goal_radius = 1.5
goal_color = g

[my_collision_checker1]
type = point_collision_checker
obstacles = [[-5.0, 0.0], [5.0, 0.0], [2.0, 10.0], [-3.0, -10.0], [-15.0, -5.0], [10.0, -10.0], [-10.0, 15.0], [10.0, 15.0], [15.0, 10.0]]
obstacles_radius = [2.0, 3.0, 7.0, 3.0, 6.0, 5.0, 4.0, 6.0, 5.0]
kinematics = my_kinematics1

[my_kinematics1]
type = point_kinematics
radius = 1

[my_observer1]
type = full_state_observer












